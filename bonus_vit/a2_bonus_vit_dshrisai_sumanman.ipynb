{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) for Image Classification [5 points]\n",
    "Use a Vision Transformer to solve the Cats and Dogs Dataset. You can use pre-defined ViT model or implement from scratch.\n",
    "Deploy the model and record a short video (~5 mins) on how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T21:23:37.539385Z",
     "iopub.status.busy": "2025-04-10T21:23:37.538698Z",
     "iopub.status.idle": "2025-04-10T21:23:37.544256Z",
     "shell.execute_reply": "2025-04-10T21:23:37.543476Z",
     "shell.execute_reply.started": "2025-04-10T21:23:37.539341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load and preprocess the dataset. This may include resizing images, normalizing pixel values, and splitting the dataset into training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T21:23:37.546317Z",
     "iopub.status.busy": "2025-04-10T21:23:37.545722Z",
     "iopub.status.idle": "2025-04-10T21:23:40.660760Z",
     "shell.execute_reply": "2025-04-10T21:23:40.659810Z",
     "shell.execute_reply.started": "2025-04-10T21:23:37.546300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T21:23:40.662139Z",
     "iopub.status.busy": "2025-04-10T21:23:40.661865Z",
     "iopub.status.idle": "2025-04-10T21:23:50.010349Z",
     "shell.execute_reply": "2025-04-10T21:23:50.009563Z",
     "shell.execute_reply.started": "2025-04-10T21:23:40.662115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/kaggle-cat-vs-dog-dataset/kagglecatsanddogs_3367a/PetImages\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(DATA_DIR, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T21:23:50.011448Z",
     "iopub.status.busy": "2025-04-10T21:23:50.011232Z",
     "iopub.status.idle": "2025-04-10T21:25:16.719106Z",
     "shell.execute_reply": "2025-04-10T21:25:16.718495Z",
     "shell.execute_reply.started": "2025-04-10T21:23:50.011429Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "valid_indices = [i for i, (x, _) in enumerate(full_dataset) if x.shape[0] == 3]\n",
    "full_dataset = torch.utils.data.Subset(full_dataset, valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T21:25:16.721004Z",
     "iopub.status.busy": "2025-04-10T21:25:16.720798Z",
     "iopub.status.idle": "2025-04-10T21:25:16.733269Z",
     "shell.execute_reply": "2025-04-10T21:25:16.732694Z",
     "shell.execute_reply.started": "2025-04-10T21:25:16.720988Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_ds, val_ds = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Choose to use a pre-defined ViT model or implement it from scratch. You can use an in-built predefined models for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T21:25:16.734069Z",
     "iopub.status.busy": "2025-04-10T21:25:16.733860Z",
     "iopub.status.idle": "2025-04-10T21:25:17.275504Z",
     "shell.execute_reply": "2025-04-10T21:25:17.274788Z",
     "shell.execute_reply.started": "2025-04-10T21:25:16.734047Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T21:25:17.276685Z",
     "iopub.status.busy": "2025-04-10T21:25:17.276341Z",
     "iopub.status.idle": "2025-04-10T21:25:17.283186Z",
     "shell.execute_reply": "2025-04-10T21:25:17.282557Z",
     "shell.execute_reply.started": "2025-04-10T21:25:17.276658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train and evaluate your ViT model. Discuss your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T21:25:17.284143Z",
     "iopub.status.busy": "2025-04-10T21:25:17.283870Z",
     "iopub.status.idle": "2025-04-10T22:01:51.971317Z",
     "shell.execute_reply": "2025-04-10T22:01:51.970477Z",
     "shell.execute_reply.started": "2025-04-10T21:25:17.284121Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1: 100%|██████████| 312/312 [11:10<00:00,  2.15s/it]\n",
      "[Val] Epoch 1: 100%|██████████| 78/78 [01:01<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary:\n",
      "  Train Loss: 0.0356 | Train Acc: 99.09%\n",
      "  Val   Loss: 0.0238 | Val   Acc: 99.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 2: 100%|██████████| 312/312 [11:10<00:00,  2.15s/it]\n",
      "[Val] Epoch 2: 100%|██████████| 78/78 [01:00<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Summary:\n",
      "  Train Loss: 0.0043 | Train Acc: 99.93%\n",
      "  Val   Loss: 0.0121 | Val   Acc: 99.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 3: 100%|██████████| 312/312 [11:10<00:00,  2.15s/it]\n",
      "[Val] Epoch 3: 100%|██████████| 78/78 [01:00<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Summary:\n",
      "  Train Loss: 0.0027 | Train Acc: 99.94%\n",
      "  Val   Loss: 0.0156 | Val   Acc: 99.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        inputs = {\"pixel_values\": images}\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"[Val] Epoch {epoch+1}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            inputs = {\"pixel_values\": images}\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_correct += (outputs.logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:01:51.973059Z",
     "iopub.status.busy": "2025-04-10T22:01:51.972443Z",
     "iopub.status.idle": "2025-04-10T22:01:52.737568Z",
     "shell.execute_reply": "2025-04-10T22:01:52.736882Z",
     "shell.execute_reply.started": "2025-04-10T22:01:51.973006Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to saved_model/\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"saved_model\", exist_ok=True)\n",
    "model.save_pretrained(\"saved_model\")\n",
    "print(\"Model saved to saved_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very High Accuracy (Train & Val)\n",
    "The model quickly reached 99%+ accuracy in the very first epoch.\n",
    "That's expected with ViT on a relatively “easy” dataset like Cats vs Dogs — the patterns are visually distinctive.\n",
    "\n",
    "Low Loss\n",
    "By epoch 3, training loss dropped below 0.003.\n",
    "Validation loss also stayed low, shows no major overfitting.\n",
    "\n",
    "Slight Increase in Val Loss at Epoch 3\n",
    "Val loss went from 0.0121 → 0.0156, while val accuracy dropped just a bit, could hint at minor overfitting about to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Deploy your trained ViT model. This could be a simple script or application that takes an image as input and predicts whether it's a cat or a dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:01:52.738618Z",
     "iopub.status.busy": "2025-04-10T22:01:52.738340Z",
     "iopub.status.idle": "2025-04-10T22:01:52.742118Z",
     "shell.execute_reply": "2025-04-10T22:01:52.741582Z",
     "shell.execute_reply.started": "2025-04-10T22:01:52.738596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, File, UploadFile\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "import io\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "model_path = \"/Users/skdharaneeshwar/Desktop/Spring25/DL/cats_dogs/saved_model\"\n",
    "model = ViTForImageClassification.from_pretrained(model_path)\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Cat vs Dog Classifier is up!\"}\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(file: UploadFile = File(...)):\n",
    "    image_bytes = await file.read()\n",
    "    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    label = \"Cat\" if predicted_class == 0 else \"Dog\"\n",
    "    return {\"prediction\": label}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(\"model_deploy:app\", host=\"127.0.0.1\", port=8000, reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Record a short video (~5 mins) demonstrating how your deployed ViT model works. The video should showcase the model taking image inputs and providing predictions. Explain the key aspects of your implementation and deployment process in the video.\n",
    "   a. Upload the video to UBbox and create a shared link\n",
    "   b. Add the link at the end of your ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shared UBbox Video Link:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://buffalo.box.com/s/ba7abdt04hajjivmnw9t6h0ouloy01h6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. References. Include details on all the resources used to complete this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face – Vision Transformer (ViT) Model:\n",
    "https://huggingface.co/google/vit-base-patch16-224\n",
    "\n",
    "PyTorch Vision ImageFolder:\n",
    "https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html\n",
    "\n",
    "FastAPI Documentation:\n",
    "https://fastapi.tiangolo.com/\n",
    "\n",
    "Uvicorn – ASGI server for FastAPI:\n",
    "https://www.uvicorn.org/\n",
    "\n",
    "Kaggle Dataset Source (Alternate):\n",
    "https://www.kaggle.com/datasets/karakaggle/kaggle-cat-vs-dog-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 630856,
     "sourceId": 1122723,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
